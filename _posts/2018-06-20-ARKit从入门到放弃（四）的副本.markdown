---
layout:     post
title:      "ARKit 从入门到放弃（三）— ARkit 官网文档翻译"
subtitle:   "欢迎来到苹果的AR世界"
date:       2018-6-20 12:00:00
author:     "NorthCity"
header-img: "img/postNew/20171206/timg.jpeg"
tags:
    - 新技术
    - 千万不要研究
    - 忘了自己是谁
---



# ARKit 从入门到放弃（三）— ARkit 官网文档翻译



## ARKit

​	整合iOS设备摄像头和运动功能，在你的应用或游戏中产生增强现实体验。



## 概述

​	增强现实(AR)描述了用户体验，即通过设备的摄像头将2D或3D元素添加到实时视图中，使这些元素看上去似乎驻留在真实世界中。ARKit结合了设备运动跟踪、相机场景捕捉、高级场景处理和显示方便，简化了构建AR体验的任务。您可以使用这些技术创建多种AR体验，使用iOS设备的后置摄像头或前置摄像头。



## 用后置摄像头增强现实 

​	最常见的AR体验是通过iOS设备的背向摄像头显示视图，并通过其他视觉内容进行增强，使用户有了一种新的方式来观察和与周围的世界互动。

​	[ARWorldTrackingConfiguration]()提供了这种体验:ARKit映射并跟踪用户所居住的真实空间，并将其与一个坐标空间相匹配，以便您放置虚拟内容。World tracking还提供了使AR体验更加身临其境的特性，例如识别用户环境中的对象和图像以及响应真实世界的灯光条件。



**注意:**

​	您可以在用户的真实环境中显示3D对象，而无需构建自定义的AR体验。在ios12中，当你在app中使用带有USDZ文件的[QLPreviewController]()，或者在web内容中使用带有USDZ文件的Safari或WebKit时，系统为3D对象提供AR视图。



## 增强现实与前置摄像头

​	在iPhone X上，[ARFaceTrackingConfiguration]()使用面向前端的TrueDepth摄像头提供有关用户面部的姿势和表情的实时信息，供您在呈现虚拟内容时使用。例如，您可以在摄像头视图中显示用户的脸，并提供真实的虚拟面具。您还可以省略摄像头视图，使用ARKit面部表情数据来动画虚拟角色，就像在iMessage的Animoji应用程序中看到的那样。



## 主题

## 第一步

[验证设备支持和用户权限]()

​	确保你的应用可以使用ARKit并且尊重用户的隐私。

[ARSession]()

​	一种共享对象，用于管理设备摄像机和增强现实体验所需的运动处理。

[ARConfiguration]()

​	AR会话配置的抽象基类。



## 显示

[ARSCNView]()

​	一种显示AR体验的视图，它通过3D SceneKit内容增强相机视图。

[ARSKView]()

​	一个显示增强现实体验的视图，用2D SpriteKit内容增强相机视图。

[展示的AR体验的材质]()。

​	通过绘制相机图像和使用位置跟踪信息显示覆盖内容来构建自定义AR视图。



## 世界跟踪

​	创建AR体验，让用户使用设备的背向摄像头探索周围世界的虚拟内容。

[建立你的第一次AR体验]()

​	创建一个运行AR会话的应用程序，使用SceneKit使用平面检测来放置3D内容。

[理解ARKit中的世界跟踪]()

​	发现支持概念、特性和最佳实践来构建优秀的AR体验。

[ARWorldTrackingConfiguration]()

​	一种配置，使用背向摄像头，跟踪设备的方向和位置，并检测真实世界的表面，以及已知的图像或对象。

[ARPlaneAnchor]()

​	关于在世界跟踪AR会话中检测到的真实平面的位置和方向的信息。

[AREnvironmentProbeAnchor]()

​	在世界跟踪AR会话中为特定区域提供环境照明信息的对象。



## 用户体验

​	通过遵循这些示例和[人类界面指南>增强现实构建]()引人注目、直观的增强现实体验。

[管理会话生命周期和跟踪质量]()

​	通过提供清晰的反馈、从中断中恢复、恢复以前的会话，使你的AR体验更加健壮。

[在增强现实中处理3D交互和UI控件。]()

​	在AR体验中，遵循视觉反馈、手势交互和真实渲染的最佳实践。



## AR世界共享与坚持

[创建多用户AR体验]()

​	使用MultipeerConnectivity framework在附近设备之间传输ARKit世界地图数据，以创建AR体验的共享基础。

[ARWorldMap]()

​	空间映射状态和来自世界跟踪AR会话的锚集。



## 图像检测和跟踪

在用户的环境中使用已知的2D图像来增强一个跟踪世界的AR会话。

[在AR体验中识别图像]()

​	在用户的环境中检测已知的2D图像，并利用它们的位置放置AR内容。

[ARReferenceImage]()

​	在世界跟踪AR会话期间在真实环境中识别的映像。

[ARImageAnchor]()

​	关于在世界跟踪AR会话中检测到的图像的位置和方向的信息。



## 对象检测

在用户的环境中使用已知的3D对象，以增强对世界的跟踪。

[扫描和检测3D物体]()

​	记录真实对象的空间特性，然后使用结果在用户环境中查找这些对象并触发AR内容。

[ARReferenceObject]()

​	在世界跟踪AR会话期间在真实环境中识别的3D对象。

[ARObjectAnchor]()

​	关于在世界跟踪AR会话中检测到的真实3D对象的位置和方向的信息。

[ARObjectScanningConfiguration]()

​	一种配置，使用后置摄像头收集高保真的空间数据，用于扫描3D对象以便以后进行检测。



## 点击测试和真实的位置

[ARHitTestResult]()

​	通过检查AR会话的设备摄像头视图中的一个点找到的真实曲面的信息。

[ARAnchor]()

​	一个真实的位置和方向，可以用来放置物体在一个AR场景。

[ARTrackable]()

​	ARKit跟踪位置和方向变化的场景中的真实对象。



## 镜头和场景的细节

[ARFrame]()

​	一个带有位置跟踪信息的视频图像，作为AR会话的一部分被捕获。

[ARCamera]()

​	关于在AR会话中捕获的视频帧的摄像机位置和成像特性的信息。

[ARLightEstimate]()

​	在AR会话中与捕获的视频帧相关联的场景照明信息的估计。



## 面部跟踪

使用iPhone X上的TrueDepth摄像头来创建AR体验，以响应用户的面部表情和面部表情。

[创建Face-Based基于“增大化现实”技术的经验]()

​	使用面部跟踪AR会话提供的信息放置和动画3D内容。

[ARFaceTrackingConfiguration]()

​	使用TrueDepth相机跟踪用户面部移动和表情的配置。

[ARFaceAnchor]()

​	面部跟踪AR会话中检测到的面部的姿态、拓扑和表情的信息。

[ARDirectionalLightEstimate]()

​	在面部跟踪AR会话中与捕获的视频帧相关的环境照明信息的估计。



## 专业配置

[AROrientationTrackingConfiguration]()

​	一种配置，使用背向摄像头，只跟踪设备的方向。

[ARImageTrackingConfiguration]()

​	一种配置，使用后置摄像头来检测和跟踪已知的图像。



## 相关技术

[用音频创造沉浸式的AR体验]()

​	使用声音效果和环境声音层创造一个迷人的AR体验。

[用ARKit实时使用视觉]()

​	管理视觉资源，有效地执行核心ML图像分类器，并使用SpriteKit在AR中显示图像分类器输出。